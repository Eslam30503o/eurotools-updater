import json
from datetime import datetime
from .utils import load_json, save_json
import time

def sync_products(products_sheet, database_file, ui=None):
    """Ù…Ø²Ø§Ù…Ù†Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ© Ù…Ø¹ Google Sheets Ø¨Ø·Ø±ÙŠÙ‚Ø© ÙØ¹Ø§Ù„Ø© (batch update)."""
    local_data = load_json(database_file, default=[])
    if isinstance(local_data, dict):
        local_data = list(local_data.values())

    try:
        rows = products_sheet.get_all_records()
        remote_by_code = {str(r.get("final_code") or "").strip(): r for r in rows}

        updated_rows = []
        appended_rows = []
        batch_updates = []

        for idx, product in enumerate(local_data, start=2):
            props = product.get("properties", {})
            template = product.get("template", "") or ""
            final_code = product.get("__final_code__") or product.get("final_code", "")

            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ù„Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯
            if not final_code:
                final_code = template
                for k, v in (props or {}).items():
                    final_code = final_code.replace(f"{{{k}}}", str(v))
                final_code = final_code.lower().replace(" ", "")
                product["__final_code__"] = final_code

            # Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙ Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„ØµØ­ÙŠØ­
            row_data = [
                product.get("name_ar", ""),
                product.get("name_en", ""),
                product.get("category", ""),
                product.get("description", ""),
                json.dumps(product.get("properties", {}), ensure_ascii=False),
                product.get("template", ""),
                final_code,
                product.get("project_name", ""),
                datetime.now().strftime("%Y-%m-%d %H:%M"),
                product.get("created_at", "")               # created_at âœ…

            ]

            # ØªØ­Ø¯ÙŠØ« Ø£Ùˆ Ø¥Ø¶Ø§ÙØ©
            if final_code in remote_by_code:
                row_index = rows.index(remote_by_code[final_code]) + 2
                batch_updates.append({"range": f"A{row_index}:J{row_index}", "values": [row_data]})
                updated_rows.append(final_code)
            else:
                appended_rows.append(row_data)

        # ğŸŸ¢ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
        if batch_updates:
            products_sheet.batch_update(batch_updates)

        # ğŸŸ¢ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„ØµÙÙˆÙ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø¯ÙØ¹Ø© ÙˆØ§Ø­Ø¯Ø©
        if appended_rows:
            products_sheet.append_rows(appended_rows)

        # âœ… Ø¹Ø±Ø¶ Ø¥Ø´Ø¹Ø§Ø±
        updated_count = len(updated_rows)
        uploaded_count = len(appended_rows)
        if ui and (uploaded_count + updated_count) > 0:
            try:
                pass
                # ui.show_toast(
                #     f"âœ… ØªÙ… Ø±ÙØ¹ {uploaded_count} ÙˆØªØ­Ø¯ÙŠØ« {updated_count} Ù…Ù†ØªØ¬ (Batch).",
                #     "success"
                # )
            except:
                pass

    except Exception as e:
        if ui:
            ui.show_toast(f"âŒ ÙØ´Ù„ Ø§Ù„Ù…Ø²Ø§Ù…Ù†Ø©: {e}", "error")
        raise


def download_products(products_sheet, database_file):
    """Ø­Ù…Ù‘Ù„ Ø§Ù„Ù…Ù†ØªØ¬Ø§Øª Ù…Ù† Google ÙˆØ­ÙØ¸Ù‡Ø§ Ù…Ø­Ù„ÙŠÙ‹Ø§."""
    rows = products_sheet.get_all_records()
    updated_data = []
    for row in rows:
        try:
            product = {
                "name_ar": row.get("name_ar", ""),
                "name_en": row.get("name_en", ""),
                "category": row.get("category", ""),
                "description": row.get("description", ""),
                "properties": json.loads(row.get("properties") or "{}"),
                "template": row.get("template", ""),
                "project_name": row.get("project_name", ""),
                "created_at": row.get("created_at", "")  # âœ… Ø£Ø¶Ù Ù‡Ø°Ø§ Ø§Ù„Ø³Ø·Ø±

            }
            updated_data.append(product)
        except Exception:
            continue

    if updated_data:
        save_json(database_file, updated_data)
